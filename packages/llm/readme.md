# llm

Useful abstractions for working with LLMs.

Features:

- synchronous and async wrappers (streaming output)
- prompt caching for supported providers
- automatically load supported providers from environment variables

We support 

- Anthropic
- Google Gemini
- OpenAI 
- OpenAI compatible providers (Together, Groq, Hyperbolic, Deepseek, ...)
